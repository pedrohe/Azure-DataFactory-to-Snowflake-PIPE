# üöÄ PIPELINE/ DATAFACTORY-->SNOWFLAKE ‚Äî PL_LOAD_BLOB_TO_SNOWFLAKE

Pipeline do **Azure Data Factory (ADF)** para carregar arquivos CSV do **Blob Storage** para o **Snowflake**. üíæ‚û°Ô∏è‚ùÑÔ∏è

---

## üß© Vis√£o Geral

* **Pipeline:** `PL_LOAD_BLOB_TO_SNOWFLAKE`
* **Fluxo:** Lookup ‚û°Ô∏è ForEach ‚û°Ô∏è Copy
* **Tabelas:** `PRODUTOS`, `CLIENTES`, `VENDAS`
* **Destino:** Snowflake (`ADF_TRAINING_DB.RAW`)
* **Staging:** Blob `adf-staging`

---

## ‚öôÔ∏è Recursos Criados (Azure)

| Tipo            | Nome Exemplo                 | Descri√ß√£o                   |
| --------------- | ---------------------------- | --------------------------- |
| Resource Group  | `rs-adf-snowflake-lab`       | Agrupa os recursos do lab   |
| Storage Account | `stgkeplerlab`               | Armazena dados e staging    |
| Containers      | `source data`, `adf-staging` | Fonte e staging do pipeline |
| Data Factory    | `adf-kepler-lab`             | Orquestra o fluxo de dados  |

---

## üîó Conex√µes (Linked Services)

* üü¶ **LS_BLOB_SOURCE** ‚Üí Blob com os arquivos CSV
* üü© **LS_BLOB_STAGING_SASURI** ‚Üí Staging (SAS URI)
* ‚ùÑÔ∏è **LS_SNOWFLAKE_DEST** ‚Üí Conex√£o com o Snowflake

> üí° Use **Azure Key Vault** para armazenar credenciais de forma segura.

---

## üìä Datasets

* **DS_ABLB_CONFIG** ‚Üí JSON com lista de arquivos/tabelas
* **DS_ABLB_SOURCE** ‚Üí Fonte CSV parametrizada (`fileName`, `folderPath`)
* **DS_SF_SINK** ‚Üí Tabela destino no Snowflake (`target_table`)

Exemplo `config.json`:

```json
[
  { "table_name": "PRODUTOS", "file": "produtos.csv" },
  { "table_name": "CLIENTES", "file": "clientes.csv" },
  { "table_name": "VENDAS", "file": "vendas.csv" }
]
```

---

## üîÑ Pipeline ‚Äî Etapas

1. üîç **Lookup_Config** ‚Üí L√™ `config.json`
2. üîÅ **ForEach** ‚Üí Itera pelas tabelas
3. üì• **Copy_To_Snowflake** ‚Üí Copia CSV ‚Üí staging ‚Üí Snowflake

   * `enableStaging: true`
   * `stagingSettings`: `LS_BLOB_STAGING_SASURI`

---

## ‚ùÑÔ∏è Provisionamento Snowflake

Execute o script abaixo no **Snowflake Worksheet** (ou via `snowsql`) para preparar o ambiente que receber√° os dados:

```sql
CREATE DATABASE IF NOT EXISTS ADF_TRAINING_DB;
CREATE SCHEMA IF NOT EXISTS RAW;
CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH 
  WITH WAREHOUSE_SIZE='XSMALL' AUTO_SUSPEND=60 AUTO_RESUME=TRUE;

USE DATABASE ADF_TRAINING_DB;
USE SCHEMA RAW;

CREATE OR REPLACE TABLE PRODUTOS (
    id_produto INT,
    nome_produto VARCHAR(100),
    preco DECIMAL(10,2)
);

CREATE OR REPLACE TABLE CLIENTES (
    id_cliente INT,
    nome VARCHAR(100),
    cidade VARCHAR(50)
);

CREATE OR REPLACE TABLE VENDAS (
    id_venda INT,
    id_cliente INT,
    id_produto INT,
    quantidade INT,
    valor_total DECIMAL(10,2)
);

-- Verifica√ß√£o das tabelas
SHOW TABLES;

-- Contagem de registros (ap√≥s ingest√£o)
SELECT COUNT(*) AS TOTAL_CLIENTES FROM CLIENTES;
SELECT COUNT(*) AS TOTAL_PRODUTOS FROM PRODUTOS;
SELECT COUNT(*) AS TOTAL_VENDAS FROM VENDAS;

-- Valida√ß√£o dos dados
SELECT * FROM VENDAS;
```

üí° **Dica:** Ap√≥s o pipeline executar com sucesso, as consultas acima devem retornar registros nas tr√™s tabelas criadas.

---

## üß† Dicas Finais

‚úÖ Utilize **Key Vault** para senhas e tokens
‚úÖ Mantenha seus CSVs em pastas organizadas (`source data/`)
‚úÖ Use o **Monitor** do ADF para acompanhar execu√ß√µes
‚úÖ Configure alertas e logs para falhas

‚ú® Ambiente pronto para ingest√£o e an√°lise no Snowflake!

